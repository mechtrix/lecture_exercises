---
title: "Big Data exercises"
author: 
  - name: Prof. Dr. Tim A. Weber
filters: 
  - webr
webr:
  packages: 
    - "dplyr"
    - "jsonlite"
    - "ggplot2"
    - "duckdb"
    - "dbplyr"
    - "janitor"
    - "rvest"
    - "readr"
editor_options: 
  chunk_output_type: console
---

```{webr-r}

#| context: setup

# Download a dataset
download.file('https://coatless.github.io/raw-data/flights.csv','flights.csv')

download.file("https://raw.githubusercontent.com/mechtrix/data/main/raw_data/mtcars.duckdb", "mtcars.duckdb")

download.file("https://raw.githubusercontent.com/mechtrix/data/refs/heads/main/raw_data/f1_drivers_wiki_page.html","f1_drivers_wiki_page.html")

```

# Getting started

## Your personal R environment

Below you can see R running in the browser. 
It is not as powerful as if we install R on the machine, but it spares us the hassle of setting R up on every of your Computers.
You can type some simple math to try it out!
You can run single lines by pressing `Ctrl` + `Enter`.

```{webr-r}
1+1
```

## loading libraries

As most open source software, R builds upon loading libraries. 
Usually those are found on [CRAN](https://cran.r-project.org/), but in our case they need to be provided.
Let's try loading libraries!

```{webr-r}
library(dplyr)
```

Here we have loaded the [dplyr](https://dplyr.tidyverse.org/) package, which is used for data manipulation.
It is part of the much bigger [tidyverse](https://www.tidyverse.org/).
Since the packages always have to be installed, we will not use the more extensive command `library(tidyverse)`.
This would trigger to install a lot of packages.
If you want to do this on your own machine, please do so

# Data Sources

For BigData we need - Data.
There are many sources out there, we will start working with a couple of them.

## data provided by R

You can simply use data that is natively provided by R.
A famous example is the titanic data set.
Do this using the `data()` command as provided below

```{webr-r}

data("Titanic")

```

There is not a lot that happened, because we do not use an IDE.
An IDE like RStudio would acutally display the variables.
This is a trade-off we have to live with the convinienct of not setting up R, but of course, there are ways around it.
Below are a couple of ways to display data:
- the `print`command is the most verbose way to output data
- the `head` command prints only the first couple of lines (can be specified using `head(object, n = X`)
- the `glimpse` command is very convenient, but needs the `dplyr` package to be loaded or referenced (with `::` as shown below)
- the `str` command give the structure of the object, so it provides a meta view of the variable. This is convenient if you have some unexpected output.

```{webr-r}
print(Titanic)
```


```{webr-r}
head(Titanic)
```


```{webr-r}
dplyr::glimpse(Titanic)
```


```{webr-r}
str(Titanic)
```


## csv

Reading a csv is fairly simple, we just need the file path.
Lucky for you this has been prepared, the file was already downloaded to the Virtual File System (VFS) that webR uses.
Classically, we can use the `read.csv()` function from baseR.
It is not as convenient, but you do not need to install or download any package to work with it.
Check out the variable content using the commands above!

```{webr-r}
flights_data_base <-  read.csv("flights.csv")
```

In the `tidyverse` we also have the [readr](https://readr.tidyverse.org/) package. 
It give more flexibility, for example specifiying columns.
We do not need to care about that now, but it is good to know.

```{webr-r}
library(readr)
flights_data_readr <-  read_csv("flights.csv")

```

But we can also explore the VFS a little by using the `getwd()` and the `list.files()` command.
There you can see the `flights.csv` file that we read in before!
When you output the complete VFS - what do you notice?

```{webr-r}
getwd()

list.files()

list.files('/', full = TRUE, recursive = TRUE)

```

## API

Below you can try to get data from an API.
This is not yet possible using webR for technical and security reasons. 
In theory, it would go like this:

This is the Give Food API at [givefood.org.uk](https://www.givefood.org.uk/api/2/docs/) and shall give us a brief introduction on how to work with API's.
We need two packages: [httr](https://CRAN.R-project.org/package=httr) and [jsonlite](https://CRAN.R-project.org/package=jsonlite).
We are pulling data from a data source that does not need a key [foodbank](https://www.givefood.org.uk/api/2/docs/).

```{r}
#| eval: false

library(httr)
library(jsonlite)

foodbank <- httr::GET("https://www.givefood.org.uk/api/2/foodbanks/")

foodbankcontent <- httr::content(foodbank, as = "text")

foodbankJSON <- jsonlite::fromJSON(foodbankcontent)

```

```
Response [https://www.givefood.org.uk/api/2/foodbanks/]
  Date: 2024-10-05 07:50
  Status: 200
  Content-Type: application/json
  Size: 1.28 MB
[
  {
    "name": "Lapford Food Bank",
    "alt_name": null,
    "slug": "lapford",
    "phone": "0136383788",
    "secondary_phone": null,
    "email": "foodbank@lapfordcc.org.uk",
    "address": "Victory Hall\r\nLapford\r\nDevon\r\nEX17 6PZ",
    "postcode": "EX17 6PZ",
...
```

But we can "fake" an API call, just to be able to play around a little with json files.
For this we will download the data that is provided by the API as a csv and re-read it into R.

```{webr-r}
library(jsonlite)

data_url <- "https://raw.githubusercontent.com/mechtrix/data/main/raw_data/foodbank"

download.file(data_url, "foodbank")

foodbankRAW <- readLines("foodbank")

foodbankJSON <- jsonlite::fromJSON(foodbankRAW)

dplyr::glimpse(foodbankJSON)

```

Ok, so now we have data, but what can we do with it?
Below is some code where we count the foodbanks according to region in the UK.

```{webr-r}
cnt_foodbank <- foodbankJSON |> 
  count(country)
```

We then add a ordered barplot so show the counts.
Piece o' cake!

```{webr-r}
cnt_foodbank |> 
  ggplot(
    aes(
      x = reorder(country,-n),
      y = n
    )
  )+
  geom_col()+
  labs(
    title = "Count of foddbanks in the UK per region",
    x = "region",
    y = "count"
  )

```

## Databases

As with API's a *real* connection to a remote database is right now not feasible using webR.
But we can use *duckDB* which is a fast in-process analytical database.
It has one more advantage: It is locally hosted, which means it is a physical file on your machine.
We also use [dbplyr](https://dbplyr.tidyverse.org/) which is a database backend that uses the same or similar logic as *dplyr* for data manipulation with remote data sources.

```{webr-r}
library(duckdb)
library(dbplyr)

ddb_con <- dbConnect(duckdb(), dbdir = "mtcars.duckdb", read_only = FALSE)

mtcars <- tbl(ddb_con,"mtcars_table") |> collect()

```

So did it work? 
You can query the object using the same commands as described above.
Another way is of course to plot it, but that is hard when we do not know what variables are in the dataset.
Sometimes in those cases, the R base plotting is an excellent way of doing some quick E(xplorative)D(ata)A(nalysis)

```{webr}

plot(mtcars)

```

## Web scraping

In this example we will scrape the F1 driver data from wikipedia, that is online [here](https://en.wikipedia.org/wiki/List_of_Formula_One_drivers).
The large table contains a lot of data that we want to download.
Again, we can not access the data from webR, that why we need to fake it a little, but the general idea on how to access a page online can be seen below.

```{r}
#| eval: false

library(rvest)
link <- "https://en.wikipedia.org/wiki/List_of_Formula_One_drivers"

page <- read_html(link)

```

We take it interactively from here.
First, we read in the html file.
Above this, you can see how that would work using an url. 
The file has been downloaded to the VFS, but we can use the same command!

```{webr-r}
library(rvest)

page <- read_html("f1_drivers_wiki_page.html")

```

Next, we need to find the right table. 
Navigate to [wikipedia page](https://en.wikipedia.org/wiki/List_of_Formula_One_drivers) and look for it.
The easiest way to do this, is by right-clicking on the page and choosing *inspect*.
You should see something like this.

![](img/find_table.png)

The table is in an table element with the sortable attribute.
The `rvest` package allow for functions that are prepared for this, so we just need to convert the raw page.
You may have notices the `clean_names()` function.
This comes from the [janitor](https://github.com/sfirke/janitor) package, which contains very useful functions for data cleaning.
It has been uploaded to your webR already, but check out the github link.
Try to alter the code to see the effect of the packagel, you have all the tools available.

```{webr-r}

drivers_F1 <- html_element(page, "table.sortable") %>%
  html_table() |> 
  clean_names() 

```

Next, we are going to do some data cleaning.
We only want to keep certain columns. 
To achieve this, we use the `select()` function from `dplyr`.

```{webr-r}
drivers_F1 <- drivers_F1 |> 
  select(
    driver_name,
    nationality,
    seasons_competed,
    drivers_championships,
    pole_positions,
    race_wins,
    podiums
  )

```

Also, please check out the last row of the dataframe.
It appears that this is no data but an explanation of the columns.
This would prevent using the data out-of-the-box.
But we do not need it anyway, so we get rid of it.
Check out the `nrow()` command by typing `?nrow` to see what it does.

```{webr-r}

drivers_F1 <- drivers_F1 |> 
   slice_head(
    n = nrow(drivers_F1)-1
  )

```

In the next step, we need to check out the column `drivers_championship`.
Apparently, in the table are the number of championships the drivers one, but also the years. 
We are (for now) only interested in the number of won championships, so we get rid of the years information by simply extracting the first character using the `substr()` function.
This is achieved by also using the `mutate()` command provided by `dplyr`, you can find more detailed references [here](https://dplyr.tidyverse.org/reference/mutate.html).

```{webr-r}

drivers_F1 <- drivers_F1 |> 
  mutate(
    drivers_championships = parse_number(substr(drivers_championships,start = 1, stop = 1))
  )

```

The next step is there for safety reasons.
We make sure with the `parse_number()` function from the `readr` package, that all numbers in the columns of interest are actual numbers.

```{webr-r}
drivers_F1 <- drivers_F1 |> 
  mutate(
    race_wins = parse_number(race_wins),
    pole_positions = parse_number(pole_positions)
  )
```

This was a step by step intro, you could also use one long chain of command as outlined below.

```{r}
#| eval: false

drivers_F1 <- drivers_F1 |> 
  select(
    driver_name,
    nationality,
    seasons_competed,
    drivers_championships,
    pole_positions,
    race_wins,
    podiums
  ) |> 
  slice_head(
    n = nrow(drivers_F1)-1
  ) |> 
  mutate(
    drivers_championships = parse_number(substr(drivers_championships,start = 1, stop = 1)),
    race_wins = parse_number(race_wins),
    pole_positions = parse_number(pole_positions)
  )

```

So, what now?
We can look into how many championships by nation have been won.
For this, we first `group_by()` nation, and then we `summarise()` the sum of all `drivers_championships`.

```{webr-r}

championships_by_nation <- drivers_F1 %>%
  group_by(
    nationality
  ) |> 
  summarise(
    championship_wins = sum(drivers_championships)
  )

```

The output is... nothing.
We need to look into the created object.

```{webr-r}
glimpse(championships_by_nation)
```

But the order is alphabetically, we want to see the top nations!

```{webr-r}

championships_by_nation |> arrange(championship_wins)

```

Oh no, wrong again, we want to have it descending.

```{webr-r}

championships_by_nation |> arrange(desc(championship_wins))

```

Ok, and who is the driver with the most won championships?

```{webr-r}

championships_by_driver <- drivers_F1 %>%
  group_by(
    driver_name
  ) |> 
  summarise(
    championship_wins = sum(drivers_championships)
  ) |> 
  arrange(desc(championship_wins))

```

Next thing we want to know is, if there is a relationship between number of `pole_positions` and the number of championships that have been won by a driver (`drivers_championsips`).

```{webr-r}

drivers_F1 %>%
  filter(pole_positions > 1) %>%
  ggplot(aes(x = pole_positions, y = drivers_championships)) +
  geom_point(position = "jitter") +
  labs(y = "Championships won", x = "Pole positions") +
  geom_smooth(
    method = "lm"
  )+
  theme_minimal()

```

So what happens in the code block?
First we take the `dataframe` and filter it, so only `driver_championships` greater than `1` are retained.
After that, we pipe it into [`ggplot2`](https://ggplot2.tidyverse.org/), a plotting system that is based on the [Grammar of Graphics](https://link.springer.com/book/10.1007/0-387-28695-0).
Onto the `x-axis` we map the number of `pole_positions` and on the `y-axis` we map the number of `driver_championships`.
This we plot with a `geom_point()`, so we tell the function to use points to represent the data.
The `position = jitter` jitters the points and prevents overplotting.
The `labs()` function assigns nice labels to the axis.
The `geom_smooth()` function maps a linear regression line (including the standard error in light gray).
In the end we can give the plot a nice theme with `theme_minimal()`.

